#Campbell Boswell & Scott Westvold
#AIM - Automated Investment Manager
#cs701 - Senior Project
#processTweet.py


"""
Filters Tweets based on content so that only relevant tweets are analyzed and
scored for sentiment. The filtered Tweets are defined as those which contain
the name or ticker symbol of a company listed on the S&P 500 or Tweets which
contain a reference to the market in general (as defined in market_keywords.py).

Possible next steps for advanced filtering which might be benificial if working
with a larger corpus of tweets include:
    - Throwing out any tweets that contain links (indicated by strings www. and
      http), as they are likely just links to articles (or spam) and will
      be difficult to pull sentiment from. This seems a little overly
      restrictive in the current twitter-sphere though, as most tweets include
      links (potentially commenting on an article), and our sample base is
      hand selected to manually filter out accounts which are prone to spam.
    - Previous works (Bollen et al. and Mittal et al.) have restricted
      tweets to those containing words/phrases such as "feel", "makes me",
      "I'm", "I am" - this could help filter both spam and difficult to
      analyze tweets, but seems overly restrictive given the realtively small
      copus of twitter users, as well as the fact that the corpus we are working
      with has been manually selected.
"""
import market_keywords
import company_filter
import company_filter_except
import config
import csv
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from pymongo import MongoClient
import string

STOP_WORDS = set(stopwords.words('english'))


def import_companies():
    '''
    A function which imports filter strings from a csv (companies.csv). As a
    means of filtering, we will build our dataset from tweets which contain
    words form one of the lists generated by this import functions.
    '''

    symbols = []
    company_name = []

    with open('companies.csv', mode='r') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        for row in csv_reader:
            symbol = row[0]
            name = row[1]

            symbols.append(("$"+symbol).lower())

            #break company names in to single words and filter for stopwords
            word_tokens = word_tokenize(name)
            for w in word_tokens:
                if w not in STOP_WORDS:
                    company_name.append(w.lower())

    return symbols, company_name


def process_tweet():
    '''
    A lengthy function which filters tweets for relevance based on their
    content. Tweets are determined to be relevant if they contain mention of a
    company name or ticker symbol of a company on the S&P500. Additionall,
    tweets which contain a reference to financial markets as determined by
    keyword filters are included.

    This function updates the contents of the mongodb database which it is
    reading tweets from. If the tweet is determined to be relevant it stores a
    processed version of the tweet which has been stripped of stopwords and
    other potentially irrelevant terms. This functionallity is not directly used
    under our lexographic sentiment analysis approach, but might be useful under
    a machine learning sentiment analysis strategy.
    '''

    #Connect to the Mongo database
    client = MongoClient('localhost', 27017) #default connection and port
    db = client['tweets'] #connect to raw tweets database

    #import sp500 companies as company names and stock ticker symbols with $
    #prepended to ticker symbols to fit expected tweet format
    symbols, company_name = import_companies()

    #cf_list and cfe_dict are used to filter out common words that appear is
    #company names (i.e. united or digital)
    cf_list = company_filter.filter_list
    cfe_dict = company_filter_except.filter_dict
    keywords = market_keywords.keywords

    #a list which store the a percentage for each user indicateing how many
    #of their tweets are flagged as relevant
    percent_relevant = []

    '''Adding twitter specific words to STOP_WORDS set'''
    STOP_WORDS.add("rt")

    '''itterate through all contentes of all mongo collections'''
    #itterate through each pre-selected twitter user (whos user names are
    #stored in the file specified by USER_LIST)
    with open(config.user_list) as csv_file:
        user_csv = csv.reader(csv_file, delimiter=',')

        #total count of how many tweets are inserted across all users
        total_tweets_processed = 0
        total_tweets_inserted = 0

        next(user_csv) #skip the first line of the file which contains lables

        for row in user_csv:
            user = row[0]
            print("Processing Tweets for @" + user)

            tweets_inserted = 0 #per user count of how many tweets are inserted
            user_tweets_processed = 0 #per user count of how many tweets are processed

            collection = db[user] #connect to user's collection in our database
            count = collection.count()
            cursor = collection.find()

            #a set of all terms which "trigger" a tweet as relevant
            trigger_word_set = set([])

            for i in range(0, count):
                doc = cursor.next()

                mongo_id = doc['_id']
                text = doc['text'].lower()

                #craft a new mongo document with processed_text field
                processed_tweet = {
                    "date": doc['date'],
                    "id": doc['id'],
                    "retweet_count": doc['retweet_count'],
                    "favorite_count": doc['favorite_count'],
                    "text": doc['text'],
                    "lang": doc['lang'],
                    "processed_text": ""
                }

                '''clean the tweet by removing any non-printable symbols & urls'''
                cleaned_text = ""
                special_word = False #twitter handles or hashtags - indicated by # or @

                for c in text:
                    if (c == '@') or (c == "#"):
                        special_word = True
                    if (c in string.whitespace):
                        special_word = False
                        cleaned_text += c
                    if (c in string.ascii_lowercase) and (special_word == False):
                        cleaned_text += c

                '''filter tweets for relevance and remove stopwords'''
                relevant_tweet = False
                processed_text = ""
                company_name_token = ""

                word_tokens = word_tokenize(cleaned_text)
                for w in word_tokens:

                    #if the tweet contains a company symbol or keyword, it's
                    #automatically relevant
                    if (w in symbols) or (w in keywords):
                        relevant_tweet = True
                        trigger_word_set.add(w)

                    #if the tweet contains a word from our list of words which
                    #comprise all company names, we first need to check if the
                    #word is not in cfe_dict and not in cf_list
                    if (w in company_name):
                        if (w not in cf_list) and (w not in cfe_dict): #i.e. not a special case...
                            relevant_tweet = True
                            trigger_word_set.add(w)

                        #if the company_name_token is not the empty string, then
                        #we know the word we just looked at was in the cfe_dict.
                        #We want to check if the current word (w) is the value
                        #associated with the key specified by company_name_token
                        #in cfe_dict. We perform this check first so that it is
                        #not triggered by the initial assignment of
                        #company_name_token, and we reset company_name_token
                        #after this if statement is triggered.
                        if company_name_token != "":
                            if w in cfe_dict[company_name_token]:
                                relevant_tweet = True
                                trigger_word_set.add(company_name_token)
                            company_name_token = "" #reset company_name_token

                        #if word w is in our dictionary of common company
                        #names, which is comprises of only the first words in
                        #company names which have multiple words and are
                        #unintentionally common (i.e. united, citizens, etc.),
                        #we want to verify that the next word (if there is a
                        #next word) is also part of the same company name.
                        if w in cfe_dict:
                            company_name_token += w

                    if (w not in STOP_WORDS) and ("http" not in w) and\
                       (w not in trigger_word_set):
                        processed_text += w + " "

                '''update processed_text field if tweet is relevant/passes our filters'''
                if relevant_tweet == True:
                    processed_tweet["processed_text"] = processed_text
                    tweets_inserted += 1
                    total_tweets_inserted += 1

                update_result = collection.replace_one(filter=doc, replacement=processed_tweet)

                user_tweets_processed += 1
                total_tweets_processed += 1

            '''Printing "success" rate'''
            print("Tweets processed: " + str(user_tweets_processed) + \
                  " Tweets inserted: " + str(tweets_inserted) +\
                  "     %" + str((tweets_inserted/user_tweets_processed)*100))

            #add to our list of user relevance percentages
            percent_relevant.append((tweets_inserted/user_tweets_processed)*100)

    store_relevance(percent_relevant)

    return total_tweets_processed, total_tweets_inserted


def store_relevance(percent_relevant):
    '''
    A simple helper function that will store data regarding the amount of tweets
    which were flagged as relevant for each user. The file that is output will
    be used in the generation of visualizations in generateVisualization.py
    '''

    with open('per_user_relevance.csv', 'w', newline='') as out_file:
        writer = csv.writer(out_file)

        with open(config.user_list) as in_file:
            reader = csv.reader(in_file, delimiter=',')
            next(reader) #skip the first line of the file which contains lables

            #itterate throught the user list and store their username along
            #with relevance percentage our writer csv
            i = 0
            for row in reader:
                user = row[0]
                percent = percent_relevant[i]
                writer.writerow([user, percent])
                i += 1



def main():
    # Process tweets
    total_tweets_processed, total_tweets_inserted = process_tweet()
    print("Total tweets processed: " + str(total_tweets_processed))
    print("Total tweets inserted: "+ str(total_tweets_inserted))

if __name__ == "__main__":
    main()
